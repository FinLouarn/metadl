<h1>MetaDL Evaluation</h1>
<p>
    For every phase, the performance of meta-learning algorithms is measured 
    through the evaluation of 600 episodes at <strong>meta-test</strong> time. 
    For each episode, your Learner produced by your meta-algorithm procedure 
    (during meta-fit), is fitted with the episode's support set (train).
    Once fitted, the Learner outputs a Predictor that will be used to predict 
    the query examples labels.
</p>
<h3>Episodes at meta-test time : We are using the 5-way 1-shot setting.</h3>
<p>
    <strong>Support set:</strong> 5 classes and 1 example per class 
    (labelled examples) <br /><strong>Query set:</strong> 5 classes and a 
    varying number of examples per class (unlabelled examples)
</p>
<h3>How to format your predictions for each phase ?</h3>
<table>
    <caption>Query description for each phase</caption>
    <tbody>
    <tr>
        <th>Phase</th>
        <th># Query set examples</th>
    </tr>
    <tr>
        <td>Public</td>
        <td>95</td>
    </tr>
    <tr>
        <td>Development</td>
        <td>TBA</td>
    </tr>
    <tr>
        <td>Final</td>
        <td>TBA</td>
    </tr>
    </tbody>
</table>

<h3>
 	Final score Metric : Sparse Categorical Accuracy.
</h3>